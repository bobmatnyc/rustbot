# Session Progress: Web Search Tool Calling Investigation

**Date**: 2025-11-15
**Focus**: Debugging why web_search tool isn't being called by LLM
**Status**: ğŸ” Enhanced logging added, awaiting user testing

## Session Overview

Continued from previous session where user reported:
- Assistant not using web_search tool for news queries
- Assistant explaining what it "would do" instead of actually searching
- User requested UI indication when agent delegation occurs

## Key Accomplishments

### 1. Enhanced Diagnostic Logging

**File**: `src/llm/openrouter.rs`

Added three levels of logging to track tool calling flow:

#### Pre-Request Logging (Lines 209-225)
```rust
// Log tools being sent to API
if let Some(ref tools) = api_request.tools {
    tracing::info!("ğŸ”§ [LLM] Sending {} tools to API:", tools.len());
    for tool in tools {
        tracing::info!("   - Tool: {}", tool.function.name);
        tracing::info!("     Description: {}", tool.function.description);
    }
}

// Log tool_choice configuration
if let Some(ref choice) = api_request.tool_choice {
    tracing::info!("ğŸ¯ [LLM] tool_choice: {:?}", choice);
} else {
    tracing::info!("ğŸ¯ [LLM] tool_choice: auto (default)");
}
```

**Purpose**: Verify tools are actually in the API request

#### Post-Response Logging (Lines 255-283)
```rust
let tool_calls = choice.message.tool_calls.as_ref().map(|calls| {
    tracing::info!("ğŸ“ [LLM] Response contains {} tool call(s)", calls.len());
    calls
        .iter()
        .filter_map(|api_call| {
            tracing::info!("   - Tool call: {} (id: {})",
                          api_call.function.name, api_call.id);
            // ... parsing logic ...
        })
        .collect()
});

if tool_calls.is_none() {
    tracing::info!("ğŸ“ [LLM] Response contains NO tool calls - LLM responded directly");
}
```

**Purpose**: **CRITICAL** - Shows whether LLM decided to call tools or not

### 2. Created Testing Infrastructure

**Scripts Created**:

1. **test_tool_calling_debug.sh** - Interactive test with user guidance
2. **analyze_logs.sh** - Automated log analysis to extract key diagnostics

**Log Analysis Sections**:
- Agent loading verification
- Tool registry status
- Tools sent to API (with descriptions)
- Tool choice configuration
- **LLM response tool calls (or lack thereof)**
- Tool execution attempts

### 3. Comprehensive Documentation

**File**: `docs/progress/2025-11-15-tool-calling-debug.md`

Complete diagnostic guide including:
- Problem summary
- Investigation results
- Enhanced logging explanation
- Testing instructions
- Diagnostic decision tree
- Potential root causes with solutions
- Next steps for each scenario

## Technical Insights

### Current Tool Flow Architecture

```
User Query
    â†“
RustbotApi.send_message()
    â†“
Primary Agent (assistant)
    - Gets tools: [web_search]
    - tool_choice: "auto"
    â†“
LlmAdapter.complete_chat()
    - Sends: messages, tools, tool_choice
    â†“
OpenRouter API
    - Claude Sonnet 4.5 processes request
    - Decides: Use tools OR Respond directly
    â†“
Response Analysis (NEW LOGGING HERE)
    - ğŸ“ Tool calls found?
      â†“ YES              â†“ NO
    Execute tool      Direct response
    web_search        (Current bug)
```

### Key Finding: tool_choice = "auto"

**Location**: `src/agent/mod.rs:425`

```rust
request.tool_choice = Some("auto".to_string());
```

**What This Means**:
- LLM has full discretion to use tools or not
- Even with strong instructions, LLM can choose to respond directly
- This explains why tools aren't being called despite proper configuration

**Alternatives**:
- `"required"`: Force tool use (might break non-tool queries)
- `"none"`: Disable tool use (not what we want)
- Dynamic: Set based on query analysis (smart approach)

### Hypothesis: Query Interpretation Issue

User's query: "What's news?"

Possible LLM interpretations:
1. âœ… **Intended**: "Search for latest news stories"
2. âŒ **Actual?**: "What is the concept of 'news'?"

More explicit queries should work better:
- "What are today's top news stories?"
- "Search the web for latest AI news"
- "What's happening with SpaceX this week?"

## Files Modified

### src/llm/openrouter.rs
- **Lines 209-225**: Added pre-request tool logging
- **Lines 255-283**: Added post-response tool call detection logging

**Purpose**: Diagnostic visibility into tool calling behavior

## Files Created

### Test Scripts
- `test_tool_calling_debug.sh` - Interactive test runner
- `analyze_logs.sh` - Log analysis automation

### Documentation
- `docs/progress/2025-11-15-tool-calling-debug.md` - Complete diagnostic guide
- `docs/progress/2025-11-15-session.md` - This file

## Current State

### âœ… Completed

1. **Root cause investigation**: Identified tool_choice and LLM decision-making as key factors
2. **Enhanced logging**: Added comprehensive diagnostic logging
3. **Testing infrastructure**: Created scripts for easy testing and log analysis
4. **Documentation**: Complete guide for debugging and next steps

### ğŸ” Awaiting User Testing

**Next Required Action**: User must:

1. Build updated code: `cargo build`
2. Run with logging: `RUST_LOG=info ./target/debug/rustbot > /tmp/rustbot_tool_debug.log 2>&1`
3. Ask test query: "What are today's top news stories?"
4. Analyze logs: `./analyze_logs.sh`
5. Report which scenario occurred:
   - **Scenario A**: "Response contains 1 tool call(s)" âœ… (Tool calling works!)
   - **Scenario B**: "Response contains NO tool calls" âŒ (LLM chose not to use tool)

### ğŸ“‹ Pending Tasks

Based on test results, one of these paths:

**If Scenario A (Tool calls work)**:
1. Investigate why user's previous test failed
2. Verify end-to-end execution with web search
3. Add UI delegation indication
4. Close issue

**If Scenario B (No tool calls)**:
1. Implement one of three solutions:
   - **Option A**: Change tool_choice to "required"
   - **Option B**: Enhance assistant instructions further
   - **Option C**: Add query-based smart tool_choice selection
2. Re-test with fix
3. Add UI delegation indication
4. Document solution

## Potential Solutions Ready

### Option A: Force Tool Use (Aggressive)

**Change**: `src/agent/mod.rs:425`
```rust
// From:
request.tool_choice = Some("auto".to_string());

// To:
request.tool_choice = Some("required".to_string());
```

**Pros**:
- Guarantees tools are used when available
- Simple one-line fix

**Cons**:
- Might force tool use for non-tool queries
- Less flexible than selective approach

### Option B: Enhanced Instructions (Gentle)

**Add to**: `agents/presets/assistant.json`
```json
"instruction": "... existing instructions ...\n\n
CRITICAL RULE: When you detect ANY of these trigger words, you MUST call
web_search immediately, no exceptions:\n
- latest, today, current, recent, now, this week\n
- What's happening, what's new, breaking, updates\n
DO NOT explain what you would search for - JUST SEARCH."
```

**Pros**:
- Maintains flexibility
- Works with tool_choice = "auto"

**Cons**:
- Relies on LLM following instructions
- May still fail with ambiguous queries

### Option C: Smart Query Analysis (Sophisticated)

**Add to**: `src/agent/mod.rs` before line 425
```rust
// Analyze query for web search trigger words
let trigger_words = ["latest", "today", "current", "recent", "now",
                     "news", "happening", "breaking"];
let requires_web_search = trigger_words.iter()
    .any(|&word| user_message.to_lowercase().contains(word));

// Set tool_choice based on query analysis
request.tool_choice = if requires_web_search {
    Some("required".to_string())  // Force tool use
} else {
    Some("auto".to_string())      // Let LLM decide
};
```

**Pros**:
- Best of both worlds: smart forcing
- Maintains flexibility for other queries
- Can be tuned with more sophisticated analysis

**Cons**:
- More complex implementation
- Keyword list needs maintenance
- Edge cases might be missed

## Next Session Preparation

For whoever continues this work:

1. **Check log analysis results** first thing
2. **Identify which scenario** occurred (A or B)
3. **Implement appropriate solution** from options above
4. **Add UI delegation indication** (separate feature)
5. **Test end-to-end** with multiple query types

## Key Learnings

### 1. FFI Boundaries and Panics

From previous session:
> Never use .expect() or panic!() in GUI framework initialization callbacks -
> they're called from FFI and panics will cause aborts instead of safe unwinding.

### 2. Tool Choice Semantics

- `"auto"`: LLM decides whether to use tools (current setting)
- `"required"`: LLM must use tools (forces tool calling)
- This is a critical parameter for controlling tool behavior

### 3. Diagnostic Logging Strategy

Adding structured logging with emojis makes log analysis much easier:
- ğŸ”§ Tool sending
- ğŸ¯ Configuration
- ğŸ“ Response analysis

This pattern should be used for other complex flows.

## Related Context

### Previous Sessions
- `2025-11-14-web-search-verification.md` - Verified system architecture
- `2025-11-13-tool-execution-complete.md` - Implemented tool execution
- `2025-11-13-web-search-plugins-fix.md` - Fixed OpenRouter API format
- `2025-11-13-panic-fix-env-loading.md` - Fixed FFI panic issue

### Agent Configurations
- `agents/presets/assistant.json` - Primary agent with strong tool instructions
- `agents/presets/web_search.json` - Web search specialist

### Code Locations
- `src/api.rs:66-77` - Tool registration
- `src/api.rs:167-186` - Tool provisioning to primary agent
- `src/agent/mod.rs:369-434` - Agent message processing with tools
- `src/agent/tools.rs:190-199` - Tool definition creation
- `src/llm/openrouter.rs:172-283` - LLM API integration with enhanced logging

## Summary

This session focused on adding diagnostic capabilities to understand why the web_search tool isn't being called. We now have:

1. âœ… **Comprehensive logging** to see exactly what's happening
2. âœ… **Testing infrastructure** to reproduce and analyze the issue
3. âœ… **Three potential solutions** ready to implement
4. ğŸ” **Awaiting user test results** to determine which solution to apply

The next session should start with log analysis and proceed directly to implementing the appropriate fix based on the results.
